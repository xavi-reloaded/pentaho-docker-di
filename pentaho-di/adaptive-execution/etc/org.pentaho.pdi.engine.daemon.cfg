# Location of Apache Spark Client distribution
sparkHome=/home/cloudera/spark-2.1.0-bin-hadoop2.7/

# Spark Karaf Assembly
sparkApp=/home/cloudera/pdi-ee-client-7.1-SNAPSHOT/data-integration

# Spark Assembly Zip
# This is the zip file of the PDI Assembly that will be used on the executors.  This can be an HDFS reference which will
# help with performance launching the application.
assemblyZip=hdfs:/opt/pentaho/pdi-ee-client-7.1-SNAPSHOT.zip

# If set > -1 Debugging will be enabled for the Spark Application
driverDebugPort=-1
executorDebugPort=-1

# If true the Spark Application will wait for a debugger to attach before executing.
suspendDebug=true

# Add extra java options for the spark driver app
sparkDriverExtraJavaOptions=-Dlog4j.configuration=file:${sparkApp}/classes/log4j.xml

# Add extra java options for the spark executors
#sparkExecutorExtraJavaOptions=-Dsome.opt=someValue

# The amount of memory to allocate to the spark driver (1g, 2g, 8g, ...)
sparkDriverMemory=4g

# The amount of memory to allocate to the spark executor (1g, 2g, 8g, ...)
sparkExecutorMemory=1g

# The amount of time in millis the Daemon will wait for the Driver program to claim the Execution
driverTimeoutMillis=240000

# Directory where *-site.xml files reside for Hadoop Configuration
hadoopConfDir=/home/cloudera/yarn-conf

# Hadoop User which will run job
hadoopUser=devuser

# Spark Master
# Where spark will run: local / yarn
# Note, you can also simulate multiple executors by using this notation (i.e. 2 executors):  local[2]
sparkMaster=local

# Deploy Mode
# This property is only used when the master is set to `yarn`.
# client  - Driver will run on the daemon machine; but the executors will run in Yarn
# cluster - Entire application will be run in cluster (Not supported yet)
sparkDeployMode=client
        
# Name of keytab used for the kerberos principal.
keytabName=devuser.keytab

# Name of the principal launching the spark application.
kerberosPrincipal=devuser

# Set to true in order to disable the proxy user.
# Disabling will cause Spark Application to run as the principal user.
disableProxyUser=false

# Set to false if you need to debug anything in the SPARK_HOME/kettleConf directory
overwriteConfig=true
